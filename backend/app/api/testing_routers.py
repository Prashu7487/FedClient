from fastapi import APIRouter, HTTPException
import os
import pandas as pd
import shutil
from utility.hdfs_services import HDFSServiceManager
from fastapi.responses import JSONResponse
import numpy as np

test_router = APIRouter(
    prefix="/test",
    tags=["Test Router"]
)

def process_parquet_and_save_xy(filename: str, session_id: str, output_column: str):
    """
    Download and combine multiple parquet files from HDFS,
    extract X and Y arrays, save them, and return metadata.

    Args:
        filename: HDFS folder name containing parquet files
        session_id: Unique session ID for temp file management
        output_column: Column to be treated as output (target)

    Returns:
        dict: Information about the combined data and saved files
    """

    # Create paths
    HDFS_PROCESSED_DATASETS_DIR = os.getenv('HDFS_PROCESSED_DATASETS_DIR')
    hdfs_path = os.path.join(HDFS_PROCESSED_DATASETS_DIR, filename)
    local_dir = os.path.join(os.getcwd(), "data")
    os.makedirs(local_dir, exist_ok=True)

    # Temporary download directory
    temp_download_dir = os.path.join(local_dir, f"temp_{session_id}")
    os.makedirs(temp_download_dir, exist_ok=True)

    # Download from HDFS
    hdfs_service = HDFSServiceManager()
    hdfs_service.download_folder_from_hdfs(hdfs_path, temp_download_dir)

    # Find and combine parquet files
    combined_df = None
    parquet_files = []

    for root, _, files in os.walk(temp_download_dir):
        for file in files:
            if file.endswith('.parquet'):
                file_path = os.path.join(root, file)
                parquet_files.append(file_path)

                df = pd.read_parquet(file_path)
                if combined_df is None:
                    combined_df = df
                else:
                    combined_df = pd.concat([combined_df, df], ignore_index=True)

    shutil.rmtree(temp_download_dir)

    if not parquet_files:
        raise Exception("No parquet files found in the downloaded folder")

    print(f"Combined DataFrame Shape: {combined_df.shape}")
    print(f"DataFrame Column Labels: {combined_df.columns.tolist()}")

    if output_column not in combined_df.columns:
        raise Exception(f"Output column '{output_column}' not found in the DataFrame")

    # Extract features and target
    X = combined_df.drop(columns=[output_column]).values
    Y = combined_df[output_column].values

    print(f"X shape: {X.shape}")
    print(f"Y shape: {Y.shape}")

    # Save to local_dir
    X_filename = os.path.join(local_dir, f"X_{session_id}.npy")
    Y_filename = os.path.join(local_dir, f"Y_{session_id}.npy")

    np.save(X_filename, X)
    np.save(Y_filename, Y)

    return {
        "message": "Data downloaded, combined, and saved successfully",
        "parquet_files_count": len(parquet_files),
        "shape": combined_df.shape,
        "columns": combined_df.columns.tolist(),
        "session_id": session_id,
        "X_filename": X_filename,
        "Y_filename": Y_filename
    }
    
@test_router.post("/download-and-combine")
async def download_and_combine(
    filename: str,
    session_id: str,
    output_column: str
):
    try:
        result = process_parquet_and_save_xy(filename, session_id, output_column)
        return JSONResponse(
            status_code=200,
            content=result
        )
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Failed to process files: {str(e)}"
        )
